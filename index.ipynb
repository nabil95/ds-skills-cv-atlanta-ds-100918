{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "Cross validation is very useful for determining optimal model parameters such as our regularization parameter alpha. It first divides the training set into subsets (by default the sklearn package uses 3) and then selects an optimal hyperparameter (in this case alpha, our regularization parameter) based on test performance. For example, if we have 3 splits: A, B and C, we can do 3 training and testing combinations and then average test performance as an overall estimate of model performance for those given parameters. (The three combinations are: Train on A+B test on c, train on A+C test on B, train on B+C test on A.) We can do this across various alpha values in order to determine an optimal regularization parameter. By default, sklearn will even estimate potential alpha for you, or you can explicit check the performance of specific alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c23d0f2efad9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Housing_Prices/train.csv')\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV, RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of X features: 37\n",
      "Model Details:\n",
      " LassoCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
      "    max_iter=1000, n_alphas=100, n_jobs=1, normalize=False, positive=False,\n",
      "    precompute='auto', random_state=None, selection='cyclic', tol=0.0001,\n",
      "    verbose=False)\n",
      "Optimal alpha: 198489.80980228688\n",
      "First 5 coefficients:\n",
      " [-2.80735194 -0.         -0.          0.25507382  0.        ]\n",
      "25\n",
      "Number of coefficients set to zero: 25\n"
     ]
    }
   ],
   "source": [
    "#Define X and Y\n",
    "feats = [col for col in df.columns if df[col].dtype in [np.int64, np.float64]]\n",
    "\n",
    "X = df[feats].drop('SalePrice', axis=1)\n",
    "\n",
    "#Impute null values\n",
    "for col in X:\n",
    "    avg = X[col].mean()\n",
    "    X[col] = X[col].fillna(value=avg)\n",
    "\n",
    "y = df.SalePrice\n",
    "\n",
    "print('Number of X features: {}'.format(len(X.columns)))\n",
    "\n",
    "#Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "L1 = LassoCV()\n",
    "print('Model Details:\\n', L1)\n",
    "\n",
    "L1.fit(X_train, y_train)\n",
    "\n",
    "print('Optimal alpha: {}'.format(L1.alpha_))\n",
    "print('First 5 coefficients:\\n', L1.coef_[:5])\n",
    "count = 0\n",
    "for num in L1.coef_:\n",
    "    if num == 0:\n",
    "        count += 1\n",
    "print(count)\n",
    "print('Number of coefficients set to zero: {}'.format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Coefficients and Using Lasso for Feature Selection\n",
    "The Lasso technique also has a very important and profound effect: feature selection. That is, many of your feature coefficients will be optimized to zero, effectively removing their impact on the model. This can be a useful application in practice when trying to reduce the number of features in the model. Note that which variables are set to zero can change if multicollinearity is present in the data. That is, if two features within the X space are highly correlated, then which takes precendence in the model is somewhat arbitrary, and as such, coefficient weights between multiple runs of `.fit()` could lead to substantially different coefficient values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of X features: 37\n",
      "Model Details:\n",
      " LassoCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
      "    max_iter=1000, n_alphas=100, n_jobs=1, normalize=True, positive=False,\n",
      "    precompute='auto', random_state=None, selection='cyclic', tol=0.0001,\n",
      "    verbose=False)\n",
      "Optimal alpha: 141.0984264427501\n",
      "First 5 coefficients:\n",
      " [ -0.00000000e+00  -5.95275404e+01   0.00000000e+00   1.60217484e-01\n",
      "   2.00649624e+04]\n",
      "21\n",
      "Number of coefficients set to zero: 21\n"
     ]
    }
   ],
   "source": [
    "#Define X and Y\n",
    "feats = [col for col in df.columns if df[col].dtype in [np.int64, np.float64]]\n",
    "\n",
    "X = df[feats].drop('SalePrice', axis=1)\n",
    "\n",
    "#Impute null values\n",
    "for col in X:\n",
    "    avg = X[col].mean()\n",
    "    X[col] = X[col].fillna(value=avg)\n",
    "\n",
    "y = df.SalePrice\n",
    "\n",
    "print('Number of X features: {}'.format(len(X.columns)))\n",
    "\n",
    "#Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "L1 = LassoCV(normalize = True)\n",
    "print('Model Details:\\n', L1)\n",
    "L1.fit(X_train, y_train)\n",
    "\n",
    "print('Optimal alpha: {}'.format(L1.alpha_))\n",
    "print('First 5 coefficients:\\n', L1.coef_[:5])\n",
    "count = 0\n",
    "for num in L1.coef_:\n",
    "    if num == 0:\n",
    "        count += 1\n",
    "print(count)\n",
    "print('Number of coefficients set to zero: {}'.format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the Mean Squarred Error \n",
    "Calculate the mean squarred error between both of the models above and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat this Process for the Ridge Regression Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Preprocessing and Feature Engineering\n",
    "Use some of our previous techniques including normalization, feature engineering, and dummy variables on the dataset. Then, repeat fitting and tuning a model, observing the performance impact compared to above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
